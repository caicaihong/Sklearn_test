#自然语言处理
1、文本表示 用标准化表示，比如句子之间的的关系，首先找一个词典库，该词典库的维度就是句子的维度，然后判断词典库的每个词在该是否句子分词后的，在则计算次数，不在则为0，这样子就形成了一个相同维度的向量，便于计算。
2、求文本的相似度，目前求文本的相似度有最短距离(欧式距离)和余弦相似度
	1.两个句子之间的距离最短说明相似度越大，应用数学的学的求距离的公式|s1-s2|这是没有方向的，也是缺点之一。
		|s1-s2|=√(x2-x1)^2+(y2-y1)^2 
	2.余弦相似度（跟两点之间的距离相比，这个是有方向的，针对性的更有说服力相似）
	  求两个点的延长线相交点的角度，越小说明相似度越大
	  s1.s2 = 两个点延长线的角度的cos值 =两点的内积/||a||.||b||
	  两点的内积 = x1.x2+y1.y2
	  ||a|| = √x1^2+y1^2
	  ||b|| =√x2^2+y2^2
	  cos(s1,s2) = x1.x2+y1.y2/(√x1^2+y1^2).(√x2^2+y2^2)
	  余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，夹角等于0，即两个向量相等，这就叫"余弦相似性"。

另外：余弦距离使用两个向量夹角的余弦值作为衡量两个个体间差异的大小。相比欧氏距离，余弦距离更加注重两个向量在方向上的差异。
加体现在方向上的差异，而不是位置。如果保持A点位置不变，B点朝原方向远离坐标轴原点，那么这个时候余弦距离 clip_image011 是保持不变的（因为夹角没有发生变化），而A、B两点的距离显然在发生改变，这就是欧氏距离和余弦距离之间的不同之处。

欧氏距离和余弦距离各自有不同的计算方式和衡量特征，因此它们适用于不同的数据分析模型：
欧氏距离能够体现个体数值特征的绝对差异，所以更多的用于需要从维度的数值大小中体现差异的分析，如使用用户行为指标分析用户价值的相似度或差异。

余弦距离更多的是从方向上区分差异，而对绝对的数值不敏感，更多的用于使用用户对内容评分来区分兴趣的相似度和差异，同时修正了用户间可能存在的度量标准不统一的问题（因为余弦距离对绝对数值不敏感）。
正因为余弦相似度在数值上的不敏感，会导致这样一种情况存在：

用户对内容评分，按5分制，X和Y两个用户对两个内容的评分分别为（1,2）和（4,5），使用余弦相似度得到的结果是0.98，两者极为相似。但从评分上看X似乎不喜欢2这个 内容，而Y则比较喜欢，（余弦相似度对数值的不敏感导致了结果的误差，需要修正这种不合理性就出现了调整余弦相似度，即所有维度上的数值都减去一个均值，比如X和Y的评分均值都是3），那么调整后为（-2，-1）和（1,2），再用余弦相似度计算，得到-0.8，相似度为负值并且差异不小，但显然更加符合现实。

那么是否可以在（用户-商品-行为数值）矩阵的基础上使用调整余弦相似度计算呢？从算法原理分析，复杂度虽然增加了，但是应该比普通余弦夹角算法要强。
由此，我们就得到了文本相似度计算的处理流程是:

  （1）找出两篇文章的关键词；

　（2）每篇文章各取出若干个关键词，合并成一个集合，计算每篇文章对于这个集合中的词的词频

　（3）生成两篇文章各自的词频向量；

　（4）计算两个向量的余弦相似度，值越大就表示越相似。
#但是这是按照count base形式计算的，并不是越多的数据该词就越重要，，比如“我们拒绝去骑车，是因为我们懒”，改句子中我们出现的次数多，不能说明该词是核心词，拒绝才是我们的核心词，所以在生成向量的时候存在很大的误差，如何把该核心词完美展现出来呢，这就出现了TF-IDF（并不是出现的越多就越重要，并不是出现的越少就越不重要）。
IDF 是考虑单词的重要性
tfidf(w) = tf(d,w)*idf(w)

tf(d,w)表示文档d中w的词频
idf(w) = logN/N(w)
N :语料库的文档总数
N（w）词语w出现在多少个文档
取log的原因是怕数据太大，反正log也是一个线性长的。选择log是最恰当的了。

总结：one-hot representation 包括以下：
{boolean（不是0就是1，不论出现多少次）、count base 计算出现的次数 、tf-IDF}
这三种办法计算向量都存在一个问题。
=#无法在语义上面进行判断单词之间的语义相似度。
缺点：
1、sparsity 稀疏性
不管是词语还是文本都是跟词典直接挂钩的。
所以就大部分都是0，只有一点点是1，因为词典库很大
2、无法表示语义

如何解决呢，那就用分布式表示方法解决
Distributed Representation 分布式表示方法是针对单词的，所以叫做词向量。
优点 自定义长度，解决了稀疏性。

那么有没有一种方法可以表示单词之间的语义相似度呢？有的，用《词向量》可以表示。

Q 100维的One-Hot表示法最多可以表达多少个不同的单词
	100个单词
Q 100维的分布式表示法最多可以表达多少个不同的单词
	可以表示正无穷的个单词
所以分布式的容纳空间是比ont-hot的表示方法大很多。

怎么学习每一个单词的分布式表示方法（词向量）
词向量代表单词的意思 word2vec 某种意义上理解成词的意思（meaning）
更好的词向量是更好的表达词的含义。

句子的向量可以由词向量得到再去计算两个句子的相似度，用余弦相似度或者欧氏距离。
计算句子的向量可以用LSTM/RNN或者平均法则计算句子向量（是由单词的向量直接算平均值）。


#倒排表
1、问答系统，判断相似度时，如果一个个与语料库计算，则复杂度太高，不好，那么如何减少时间复杂度呢？
核心思路：层次过滤思想
如果采用层次过滤思想，则需要复杂度是递增的。（复杂度）过滤器1<（复杂度）过滤器2<（复杂度）过滤器3(余弦相似度)
倒排表可以快速找到“词”是属于哪个文章的

然后就可以进行过滤使用（倒排表），然后进行排序。


#Noisy Channel Model  （就如下面应用场景得到的值都需要应用到语言模型，这就叫做Noisy Channel Model） 
P(text|source) ~~ P(source|text)P(text)
通过贝叶斯公式可知：P(text|source) ~~ P(source|text)P(text) / P(source)   ==P(text)就是用到语言模型
由于P(source)是常数，可以忽略
应用场景：语音识别、机器翻译、拼写纠错、OCR、密码破解   == 都有一个共同点，就是给定一个信号，就得到一个文本

#语言模型
语言模型用来判断：是否一句话从语法上通顺
语言模型可以帮忙判断这条句子是否通顺 所以语言模型是很重要的。
#Chain Rule
联合概率和条件概率转换
p(a.b) = p(a|b).p(b) = p(b|a).p(a)
当拥有n个元素的时候，我们就叫做P(A).P(B|A).P(C|AB).P(D|ABC) == chain rule
P(A.B.C.D) = P(A).P(B|A).P(C|AB).P(D|ABC)

Chain Rule和马尔可夫模型很像，马尔可夫模型就是建立在Chain Rule的基础上
但马尔可夫模型有1st order 、2nd order 和n order （就是多少阶依赖，比如一阶就是所求的只跟前面一个有关，后面类似推理），而Chain Rule是不断递增跟前面相关。

#语言模型
种类：uni-gram->bi-gram->n-gram 从简单到难
unigram 是只求每个元素的边缘概率，都是独立的，不依赖其他的元素。
P(W1,W2,W3,W4,W5,W6,W7,W8,W9) = P(W1)P(W2)P(W3)P(W4)P(W5)P(W6)P(W7)P(W8)P(W9)
unigram缺点 没有语义之间的区别。所以很难区别两个句子是否通顺。如果所有单词都一样，顺序不一样，其实概率一样，那这样子句子的概率一样，那这样子就是不合理的。

bigram 是基于马尔可夫模型下的“一阶依赖”求概率（大部分都是应用这个语言模型）

n-gram 更高级的语言模型，就是说在语义上面更好，依赖性更强，n>2
n=3就是二阶马尔可夫模型。

#估计语言模型的概率
uni-gram 就是计算每个词的词频/总共的词
bi-gram 一阶的依赖概率在单词里面计算，比如算p(是|明天)
n-gram 跟bi-gram类推

#评估语言模型 通常使用preplexity评估方法去评估 
理想的情况下：
1、假设有两个语言模型 A,B
2、选定一个特定的任务比如拼写纠错
3、把两个模型A,B都应用在此任务中
4、最后比较准确率，从而判断A,B的表现

Preplexity = 2^(-x)  x:average log likelihood
Preplexity越小越好 x = 就是每个单词的概率加log，全部的log加起来/全部词的个数，再带进去preplexity得到的值。

smoothing(平滑方法)：add-one smoothing/add-k smoothing / interpolation/good-turning smoothing
平滑的操作：为了不让之出现一堆0的情况，则计算方式在分子+1，分母加V（语料库）

interpolation 会把uni-gram的情况也考虑进去，核心思路：在计算Trigram概率时同时考虑Unigram，Bigram，Trigram出现的频次。


书写动态规划



#拼写纠错